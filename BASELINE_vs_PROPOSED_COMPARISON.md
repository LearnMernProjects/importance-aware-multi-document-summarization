# BASELINE vs PROPOSED: What's THE SAME vs DIFFERENT

## ‚úÖ YES - SAME COMPONENTS

### **1. Summarization Model (IDENTICAL)**
```
Both use: facebook/bart-large-cnn

Parameters:
- Max input tokens: 1024
- Output length: 150 tokens (min 30)
- No fine-tuning applied
```

**Code Proof:**
- Baseline: `generate_baseline_summaries.py` - Line 53: `"Loading facebook/bart-large-cnn..."`
- Proposed: `generate_proposed_summaries.py` - Line 195: `"Loading facebook/bart-large-cnn..."`

---

### **2. Evaluation Metrics (IDENTICAL)**
```
Both evaluated using:
- ROUGE-1 (Unigram overlap)
- ROUGE-2 (Bigram overlap)
- ROUGE-L (Longest common subsequence)
- BERTScore F1 (Semantic similarity)
```

**Code Proof:**
- Baseline: `evaluate_baseline.py` - Lines 34-37
- Proposed: `evaluate_proposed.py` - Lines 34-37 (IDENTICAL code)

---

### **3. Evaluation Methodology (IDENTICAL)**
```
Both:
- Use same reference summaries
- Apply ROUGE scorer with stemmer=True
- Compute BERTScore on GPU when available
- Generate identical visualizations and statistics
```

---

## ‚ùå DIFFERENT COMPONENTS

### **The ONLY Difference: INPUT DOCUMENT ORDER**

#### **BASELINE APPROACH:**
```
Step 1: Load articles from cluster
        [Article_1] (published Jan 1)
        [Article_2] (published Jan 1)
        [Article_3] (published Jan 2)

Step 2: Sort by publication date (CHRONOLOGICAL ORDER)
        [Article_1] ‚Üí [Article_2] ‚Üí [Article_3]

Step 3: Combine with separator
        "Document 1:\n{text1}\n\nDocument 2:\n{text2}\n\nDocument 3:\n{text3}"

Step 4: Feed to BART
        BART(chronological_text) ‚Üí Summary
```

**Code Location:** `generate_baseline_summaries.py` Lines 38-75
```python
def prepare_cluster_text(articles):
    """Combine multiple articles for multi-document summarization"""
    combined = "\n\n".join([f"Document {i+1}:\n{text}" for i, text in enumerate(articles)])
    return combined
```

---

#### **PROPOSED APPROACH (YOUR INNOVATION):**
```
Step 1: Load articles from cluster
        [Article_1] (published Jan 1)
        [Article_2] (published Jan 1)
        [Article_3] (published Jan 2)

Step 2: Compute semantic embeddings
        h_1 = encode(headline_1 + text_1)  [384-dim vector]
        h_2 = encode(headline_2 + text_2)  [384-dim vector]
        h_3 = encode(headline_3 + text_3)  [384-dim vector]

Step 3: Compute importance scores (CENTRALITY)
        Œ±_1 = mean(sim(h_1, h_2), sim(h_1, h_3))
        Œ±_2 = mean(sim(h_2, h_1), sim(h_2, h_3))
        Œ±_3 = mean(sim(h_3, h_1), sim(h_3, h_2))

Step 4: Normalize weights
        w_1 = softmax(Œ±_1)
        w_2 = softmax(Œ±_2)
        w_3 = softmax(Œ±_3)

Step 5: Sort by importance (descending)
        sorted_order = [Article_2, Article_1, Article_3]  (if w_2 > w_1 > w_3)

Step 6: Combine with separator
        "Document 1:\n{text_2}\n[DOCUMENT BOUNDARY]\nDocument 2:\n{text_1}\n[DOCUMENT BOUNDARY]\nDocument 3:\n{text_3}"

Step 7: Feed to BART
        BART(importance_ordered_text) ‚Üí Summary
```

**Code Location:** `generate_proposed_summaries.py` Lines 62-138
```python
# Compute embeddings h_i
h_i = embedding_model.encode(combined_texts, convert_to_numpy=True)

# Compute centrality scores Œ±_i
similarity_matrix = cosine_similarity(h_i)
np.fill_diagonal(similarity_matrix, 0)
alpha_i = np.mean(similarity_matrix, axis=1)

# Normalize w_i
w_i = softmax(alpha_i)

# Sort by importance
sorted_indices = np.argsort(w_i)[::-1]
```

---

## üìä Summary: What Changed vs What Stayed Same

| Component | Baseline | Proposed | Same? |
|-----------|----------|----------|-------|
| **Summarization Model** | facebook/bart-large-cnn | facebook/bart-large-cnn | ‚úÖ YES |
| **ROUGE Metrics** | ROUGE-1, 2, L | ROUGE-1, 2, L | ‚úÖ YES |
| **BERTScore Metric** | Yes | Yes | ‚úÖ YES |
| **Reference Summaries** | Same dataset | Same dataset | ‚úÖ YES |
| **Dataset** | NewsSumm (25 clusters) | NewsSumm (25 clusters) | ‚úÖ YES |
| **Document Order** | **Chronological** | **Importance-weighted** | ‚ùå **DIFFERENT** |
| **Article Weighting** | None (binary) | Softmax probabilities | ‚ùå **DIFFERENT** |
| **Embedding Used** | None | all-MiniLM-L6-v2 | ‚ùå **DIFFERENT** |

---

## üéØ The Fair Comparison

This is what makes your research **scientifically valid**:

‚úÖ **Controlled Variable**: Only document order changes  
‚úÖ **Same Model**: Both use identical BART  
‚úÖ **Same Evaluation**: Both use identical metrics  
‚úÖ **Same Data**: Both use identical test set  

**Result**: Any performance difference = PURELY from importance-weighted ordering, not from model differences!

---

## üí° Why This Methodology is Strong

```
Scientific Question: "Does importance-weighted ordering improve multi-document summarization?"

Control: Keep everything the same
         ‚úì Same BART model
         ‚úì Same evaluation metrics
         ‚úì Same dataset

Variable: Change only the input order
         ‚úó Baseline: chronological
         ‚úì Proposed: importance-weighted

Conclusion: Differences are due to ordering, not any other factor
```

---

## üîç Your Contribution Proven

You're not just using a different model. You're testing a **specific hypothesis**:

**Hypothesis**: "Ordering articles by semantic centrality before summarization improves multi-document quality"

**Result**: 
- ‚úÖ TRUE for hierarchical news (+27-33%)
- ‚ùå FALSE for breaking news (-39%)
- ü§î NEUTRAL overall (+0.54%)

This is **valid research** because:
1. Controlled comparison (same model, metrics, data)
2. Clear methodology (importance = semantic centrality)
3. Honest reporting (shows both successes and failures)
4. Category-wise analysis (explains why)
