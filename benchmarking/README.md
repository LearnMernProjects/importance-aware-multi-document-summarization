# Comprehensive Multi-Document Summarization Benchmarking Pipeline

A complete, reproducible framework for training, evaluating, and comparing **11 different multi-document summarization models** on the **NewsSumm dataset**.

## ğŸ“‹ Overview

This pipeline implements a rigorous benchmarking system that:

âœ… **Trains** all models on identical, preprocessed multi-document clusters  
âœ… **Evaluates** using 9 comprehensive metrics (ROUGE, BERTScore, redundancy, omission, hallucination, faithfulness)  
âœ… **Compares** results with statistical significance testing and bootstrap confidence intervals  
âœ… **Produces** publication-ready visualizations and comparison tables  
âœ… **Ensures** reproducibility through fixed seeds and detailed logging  

## ğŸ¯ Models Implemented

### Baseline Models (Transformer-based)
1. **PEGASUS** - facebook/pegasus-arxiv
2. **LED** - Longformer-Encoder-Decoder (allenai/led-base-16384)
3. **BigBird-Pegasus** - google/bigbird-pegasus-large-arxiv
4. **PRIMERA** - allenai/primera (pre-trained on multi-doc)
5. **LongT5** - google/long-t5-tglobal-base

### Advanced Models
6. **GraphSum** - Graph-based inter-document summarization
7. **Instruction-tuned LLM** - Mistral-7B-Instruct
8. **Factuality-aware Framework** - Generate + Verify with NLI
9. **Event-aware Clustering** - Event detection + importance weighting
10. **Benchmark LLM** - GPT2/general LLM baseline

### Reference System
11. **AIMS** - Article-level Importance-aware Multi-document Summarization (ğŸ“Œ **Novel Contribution**)

## ğŸ“Š Evaluation Metrics

| Metric | Definition | Better | Range |
|--------|-----------|--------|-------|
| **ROUGE-1/2/L** | N-gram overlap (F1 score) | Higher | [0, 1] |
| **BERTScore-F1** | Contextual semantic similarity | Higher | [0, 1] |
| **Redundancy Rate** | Repeated content fraction | Lower | [0, 1] |
| **Omission Rate** | Missing entities fraction | Lower | [0, 1] |
| **Hallucination Rate** | Fabricated content fraction | Lower | [0, 1] |
| **Faithfulness** | 1 - Hallucination Rate | Higher | [0, 1] |
| **Compression Ratio** | Generated/source word ratio | Context-dependent | [0, âˆ] |

## ğŸš€ Quick Start

### 1. Installation

```bash
cd benchmarking
pip install -r requirements.txt

# Download spaCy model for NER
python -m spacy download en_core_web_sm
```

### 2. Prepare Dataset

```bash
python -m data.dataset
```

This will:
- Load NewsSumm from `../data/processed/newssumm_clean.csv`
- Cluster articles by real-world events (semantic + temporal constraints)
- Split into train (80%), validation (10%), test (10%)
- Save JSON clusters for training/evaluation

### 3. Run Full Pipeline

```bash
python run_benchmarking.py
```

This will:
- Train all models with early stopping based on validation BERTScore
- Evaluate all models on test set
- Compute all 9 metrics for each summary
- Perform statistical significance testing
- Generate publication-ready plots and tables

### 4. View Results

```bash
cd results/
# Main results table
cat results.csv

# AIMS vs baseline comparison
cat aims_vs_all_comparison.csv

# Statistical significance report
cat statistical_report.json

# Plots (PNG files)
ls *.png
```

## ğŸ“ Project Structure

```
benchmarking/
â”œâ”€â”€ config.py                    # Global hyperparameters
â”œâ”€â”€ run_benchmarking.py         # Main orchestration script
â”œâ”€â”€ DOCUMENTATION.py            # Full documentation
â”œâ”€â”€ requirements.txt            # Dependencies
â”‚
â”œâ”€â”€ data/
â”‚   â””â”€â”€ dataset.py             # Data loading, clustering, splitting
â”‚
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ base.py                # Abstract base classes
â”‚   â”œâ”€â”€ pegasus_model.py       # PEGASUS implementation
â”‚   â”œâ”€â”€ led_model.py           # LED implementation
â”‚   â”œâ”€â”€ aims_model.py          # AIMS implementation
â”‚   â””â”€â”€ ...                    # Other model implementations
â”‚
â”œâ”€â”€ training/
â”‚   â””â”€â”€ trainer.py             # Unified training loop with early stopping
â”‚
â”œâ”€â”€ evaluation/
â”‚   â”œâ”€â”€ metrics.py             # 9 comprehensive metrics
â”‚   â”œâ”€â”€ statistics.py          # Bootstrap testing & significance
â”‚   â””â”€â”€ visualization.py       # Publication-ready plots
â”‚
â”œâ”€â”€ utils/
â”‚   â””â”€â”€ utils.py              # Utilities (logging, reproducibility)
â”‚
â”œâ”€â”€ checkpoints/               # Model weights after training
â”‚   â”œâ”€â”€ pegasus/
â”‚   â”œâ”€â”€ led/
â”‚   â”œâ”€â”€ aims/
â”‚   â””â”€â”€ ...
â”‚
â””â”€â”€ results/                   # Final outputs
    â”œâ”€â”€ results.csv           # All metrics for all models
    â”œâ”€â”€ aims_vs_all_comparison.csv
    â”œâ”€â”€ statistical_report.json
    â”œâ”€â”€ per_sample_results.json
    â””â”€â”€ plots/                # Visualizations
```

## ğŸ”§ Configuration

Key settings in `config.py`:

### Dataset
```python
DATASET_CONFIG = {
    "split_ratios": {"train": 0.8, "val": 0.1, "test": 0.1},
    "min_cluster_size": 2,
    "max_cluster_size": 20,
    "similarity_threshold": 0.60,
}
```

### Training
```python
TRAINING_CONFIG = {
    "batch_size": 4,
    "num_epochs": 3,
    "learning_rate": 2e-5,
    "early_stopping_patience": 3,
    "seed": 42,
}
```

### Inference
```python
INFERENCE_CONFIG = {
    "max_length": 256,
    "num_beams": 4,
    "repetition_penalty": 2.0,
}
```

## ğŸ“ˆ Understanding AIMS

**AIMS** (Article-level Importance-aware Multi-document Summarization) is the novel reference system.

### Algorithm
1. **Encode Articles**: `h_i = embedding_model(article_i)` (384-dim vectors)
2. **Compute Centrality**: `Î±_i = mean(cosine_similarity(h_i, h_j) for j â‰  i)`
3. **Normalize Weights**: `w_i = softmax(Î±_i)`
4. **Order Articles**: Sort by importance (descending)
5. **Summarize**: `summary = summarizer(concatenate(sorted_articles))`

### Advantages
- **Principled importance weighting** based on semantic similarity
- **Cross-document perspective** - articles important relative to cluster
- **No external parameters** - fully unsupervised
- **Theoretically grounded** - softmax normalization ensures valid probability distribution
- **Interpretable** - can visualize importance scores

## ğŸ“Š Expected Results

Typical results on NewsSumm (test set, ~200 clusters):

| Model | ROUGE-1 | ROUGE-2 | ROUGE-L | BERTScore-F1 |
|-------|---------|---------|---------|--------------|
| PEGASUS | 0.52 | 0.28 | 0.48 | 0.92 |
| LED | 0.54 | 0.30 | 0.50 | 0.93 |
| PRIMERA | 0.55 | 0.31 | 0.51 | 0.94 |
| **AIMS** | **0.57** | **0.33** | **0.53** | **0.95** |

(*Note: These are example ranges - actual results depend on hyperparameters and random seeds*)

## ğŸ§ª Evaluation Protocol

### Fair Comparison Guarantees
âœ… **Same test set** - All models evaluated on identical test clusters  
âœ… **Same preprocessing** - Identical tokenization and input formatting  
âœ… **Same inference parameters** - Identical beam size, length penalty, etc.  
âœ… **Same seeds** - Reproducible results  
âœ… **Statistical rigor** - Bootstrap confidence intervals + significance tests  

### Statistical Testing
- **Bootstrap CI**: 10,000 bootstrap samples
- **Paired tests**: AIMS vs each baseline
- **P-value**: Two-tailed, Î± = 0.05
- **Effect size**: Cohen's d (standardized mean difference)

## ğŸ“Š Output Files

After running the pipeline:

```
results/
â”œâ”€â”€ results.csv                     # Main table (all models Ã— all metrics)
â”œâ”€â”€ summary_results.csv             # Key metrics only
â”œâ”€â”€ aims_vs_all_comparison.csv      # AIMS improvement % over baselines
â”œâ”€â”€ statistical_report.json         # Significance tests + rankings
â”œâ”€â”€ per_sample_results.json         # Per-summary metric details
â”‚
â””â”€â”€ plots/
    â”œâ”€â”€ comparison_rouge1_mean.png
    â”œâ”€â”€ comparison_rouge2_mean.png
    â”œâ”€â”€ comparison_bertscore_f1_mean.png
    â”œâ”€â”€ metrics_heatmap.png
    â”œâ”€â”€ aims_improvement.png
    â”œâ”€â”€ distribution_bertscore_f1.png
    â””â”€â”€ radar_chart.png
```

## ğŸ” Example Usage

### Train a single model
```python
from models.pegasus_model import PEGASUSSummarizer
from training.trainer import PEGASUSTrainer
from data.dataset import prepare_dataset

train, val, test = prepare_dataset()

model = PEGASUSSummarizer(device="cuda")
trainer = PEGASUSTrainer(model, "pegasus", device="cuda")

results = trainer.train(
    train_clusters=train[:100],
    val_clusters=val[:20],
    num_epochs=3,
    batch_size=4
)
```

### Evaluate on test set
```python
from evaluation.metrics import EvaluationEngine

evaluator = EvaluationEngine(device="cuda")

documents = ["Article 1", "Article 2"]
reference = "Reference summary"
generated = model.generate_summary(documents)

metrics = evaluator.evaluate_single(documents, reference, generated)
print(f"ROUGE-1: {metrics['rouge1']:.4f}")
print(f"BERTScore: {metrics['bertscore_f1']:.4f}")
print(f"Faithfulness: {metrics['faithfulness']:.4f}")
```

### Generate statistical comparison
```python
from evaluation.statistics import ComparisonSummary, generate_statistical_report

# Per-sample results from all models
per_sample = {
    "pegasus": {"rouge1": [0.5, 0.52, ...], ...},
    "led": {"rouge1": [0.54, 0.55, ...], ...},
    "aims": {"rouge1": [0.57, 0.56, ...], ...},
}

generate_statistical_report(per_sample, Path("results/statistical_report.json"))
```

## âš™ï¸ Advanced Configuration

### GPU Memory Management
```python
# For smaller GPU (<12GB):
TRAINING_CONFIG["batch_size"] = 2
TRAINING_CONFIG["gradient_accumulation_steps"] = 4

# For larger GPU (24GB+):
TRAINING_CONFIG["batch_size"] = 8
TRAINING_CONFIG["gradient_accumulation_steps"] = 1
```

### Extend with New Models
1. Create model class inheriting from `BaseSummarizer`
2. Implement `load_model()`, `train_step()`, `validation_step()`, `generate_summary()`
3. Add to `models/your_model.py`
4. Register in `run_benchmarking.py`
5. Add config to `MODELS_TO_BENCHMARK` in `config.py`

## ğŸ”¬ Reproducibility

This pipeline prioritizes reproducibility:

```python
# All runs are deterministic
set_seed(42)  # Sets torch, numpy, random, cuDNN seeds
```

Same setup â†’ Same results guaranteed.

## ğŸ“ Citation

If you use this framework or AIMS:

```bibtex
@software{benchmarking2025,
  title={Comprehensive Multi-Document Summarization Benchmarking Framework},
  year={2025}
}

@inproceedings{aims2025,
  title={AIMS: Article-level Importance-aware Multi-document Summarization},
  year={2025}
}
```

## ğŸ“ Support

For issues, questions, or contributions:
1. Check `DOCUMENTATION.py` for detailed explanations
2. Review model implementations in `models/`
3. Check training logs in `checkpoints/<model>/training_logs.json`
4. View benchmark logs in `benchmarking.log`

## ğŸ“„ License

[Your License Here]

---

**Last Updated**: January 2025  
**Framework Version**: 1.0.0  
**Python**: 3.10+  
**PyTorch**: 2.0+  
**Transformers**: 4.30+
