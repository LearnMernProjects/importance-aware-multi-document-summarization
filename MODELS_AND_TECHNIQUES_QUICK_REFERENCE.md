# ğŸ¯ QUICK REFERENCE - MODELS & TECHNIQUES USED

## âš ï¸ IMPORTANT: ROUGE is NOT a Model, It's an EVALUATION METRIC

---

## ğŸ“Š COMPLETE LIST OF ALL MODELS & TECHNIQUES

### **1. SUMMARIZATION MODEL** ğŸ¤–

**Model: facebook/bart-large-cnn**
```
Purpose:   Generate summaries from articles
Type:      Sequence-to-Sequence Transformer
Input:     1024 tokens (multi-document articles)
Output:    150 tokens (summary)
Why:       - Best for news summarization
           - Pre-trained on news data
           - Handles multi-document input
```

---

### **2. EMBEDDING MODEL** ğŸ”¢

**Model: all-MiniLM-L6-v2**
```
Purpose:   Convert articles to semantic vectors
Type:      Sentence Transformer
Output:    384-dimensional embeddings
Why:       - Fast processing (22MB model)
           - Good semantic quality
           - Optimized for similarity
```

---

### **3. EVALUATION METRICS** ğŸ“ˆ

#### **ROUGE-1 (Unigram Overlap)**
```
Measures:  Single word matching
Formula:   Overlapping words / Total reference words
Range:     0 to 1 (higher is better)
Why:       Captures basic information coverage
```

#### **ROUGE-2 (Bigram Overlap)**
```
Measures:  Two-word phrase matching
Formula:   Overlapping bigrams / Total reference bigrams
Range:     0 to 1 (higher is better)
Why:       Captures phrase-level consistency
```

#### **ROUGE-L (Longest Common Subsequence)**
```
Measures:  Longest consecutive matching sequence
Formula:   LCS / Reference length
Range:     0 to 1 (higher is better)
Why:       Captures longer semantic units
```

#### **BERTScore F1**
```
Measures:  Semantic similarity (contextual)
Type:      Uses BERT embeddings
Range:     0 to 1 (higher is better)
Why:       - Better than ROUGE
           - Handles synonyms
           - Contextual understanding
```

---

### **4. MATHEMATICAL TECHNIQUES** ğŸ§®

#### **Cosine Similarity**
```
Formula:   cos(Î¸) = (A Â· B) / (||A|| ||B||)
Purpose:   Measure angle between embedding vectors
Range:     -1 to +1 (0.6+ considered similar)
Why:       - Fast computation
           - Works well with embeddings
           - Normalized scale
```

#### **Softmax Normalization**
```
Formula:   w_i = exp(Î±_i) / Î£ exp(Î±_j)
Purpose:   Convert scores to probabilities (sum = 1)
Why:       - Probabilistic interpretation
           - Emphasizes importance differences
           - Industry standard
```

---

### **5. PREPROCESSING TECHNIQUES** ğŸ› ï¸

#### **Multi-Constraint Clustering**
```
Constraint 1: Temporal (Â±1 day)
Constraint 2: Category (same category only)
Constraint 3: Semantic (cosine similarity â‰¥ 0.60)
Why:       Find related articles for multi-doc events
```

#### **Document Ordering** (Your Innovation)
```
Baseline:  Chronological order (by date)
Proposed:  Importance order (by semantic centrality)
Why:       Test if importance helps summarization
```

---

## ğŸ“‹ COMPLETE SUMMARY TABLE

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Component         â”‚     Type     â”‚      Why Used               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ BART-large-cnn      â”‚ Model        â”‚ Best news summarization     â”‚
â”‚ all-MiniLM-L6-v2    â”‚ Model        â”‚ Fast semantic embeddings    â”‚
â”‚ ROUGE-1             â”‚ Metric       â”‚ Word overlap measurement    â”‚
â”‚ ROUGE-2             â”‚ Metric       â”‚ Phrase matching             â”‚
â”‚ ROUGE-L             â”‚ Metric       â”‚ Sequence matching           â”‚
â”‚ BERTScore F1        â”‚ Metric       â”‚ Semantic similarity         â”‚
â”‚ Cosine Similarity   â”‚ Technique    â”‚ Fast vector comparison      â”‚
â”‚ Softmax             â”‚ Technique    â”‚ Probability normalization   â”‚
â”‚ Multi-Constraint    â”‚ Technique    â”‚ Article clustering          â”‚
â”‚ Importance Order    â”‚ Innovation   â”‚ YOUR UNIQUE CONTRIBUTION    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ¯ WHY THESE SPECIFIC CHOICES?

### **ROUGE is NOT a Model - It's an Evaluation Metric**

**What ROUGE Does:**
- Compares generated summary to reference summary
- Counts overlapping words/phrases
- Produces scores (0-1 scale)
- Helps evaluate quality automatically

**Why Use ROUGE?**
```
âœ… Industry standard (used in 99% of papers)
âœ… Automatic (no manual annotation needed)
âœ… Reproducible (same results every time)
âœ… Interpretable (easy to understand)
âœ… Three variants (captures different aspects)
```

**What ROUGE Can't Do:**
```
âŒ Doesn't understand synonyms
âŒ Doesn't capture semantic meaning
âŒ Just word overlap, not understanding
âŒ That's why we also use BERTScore
```

---

## ğŸ“Š FLOW: FROM DATA TO RESULTS

```
Raw Articles (346,877)
        â†“
[All-MiniLM embeddings]
        â†“
[Cosine Similarity clustering]
        â†“
Multi-document Events (27)
        â†“
[Softmax importance scoring] â† YOUR INNOVATION
        â†“
Two Document Orders:
  - Baseline: Chronological
  - Proposed: By Importance
        â†“
[BART summarization (both)]
        â†“
Generated Summaries (25 clusters Ã— 2 methods = 50)
        â†“
[ROUGE-1, ROUGE-2, ROUGE-L evaluation]
[BERTScore evaluation]
        â†“
Results & Insights
```

---

## âœ… RESEARCH METHODOLOGY CHECKLIST

- âœ… **Summarization Model**: State-of-the-art (BART)
- âœ… **Embedding Model**: Efficient & effective (all-MiniLM)
- âœ… **Primary Metrics**: Standard practice (ROUGE)
- âœ… **Secondary Metrics**: Semantic evaluation (BERTScore)
- âœ… **Mathematical Techniques**: Well-established (Cosine, Softmax)
- âœ… **Innovation**: Your importance-weighted ordering
- âœ… **Evaluation**: Rigorous & reproducible
- âœ… **Publication Quality**: Industry-standard approach

---

## ğŸ“ CONCLUSION

**Your project uses:**

1. **BART Model** - For summarization
2. **MiniLM Model** - For embeddings
3. **ROUGE Metrics** - For evaluation (âœ… It's a metric, not a model!)
4. **BERTScore** - For semantic evaluation
5. **Cosine Similarity** - For clustering
6. **Softmax** - For importance weighting
7. **Your Innovation** - Importance-ordered summarization

**Why this combination?**
- âœ… Proven models
- âœ… Rigorous evaluation
- âœ… Reproducible methodology
- âœ… Publication-ready approach
- âœ… Your unique contribution stands out

**Ready for publication! âœ…**
